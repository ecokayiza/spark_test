{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/26 00:58:02 WARN Utils: Your hostname, user-System-Product-Name resolves to a loopback address: 127.0.1.1; using 114.213.214.100 instead (on interface enp36s0f1)\n",
      "24/12/26 00:58:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/26 00:58:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,count,mean,udf,sum,when\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Typhoon Analyze\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "spark.conf.set(\"spark.rapids.sql.enable\",\"true\")\n",
    "\n",
    "\n",
    "df_grade=spark.read.option(\"header\", True).csv(r\"../design/result/grade_trend/part-00000-7dcf4de8-72b3-42bb-bf65-4d2d581f866e-c000.csv\")\n",
    "df_intensity=spark.read.option(\"header\", True).csv(r\"../design/result/intensity_trend/part-00000-230f148b-8c77-4f42-bc0c-4d0236d48799-c000.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------------+\n",
      "|year|avg_central_pressure|    avg_wind_speed|\n",
      "+----+--------------------+------------------+\n",
      "|1977|   986.1454311454312|31.975546975546976|\n",
      "|1978|   986.0371747211896| 34.02416356877323|\n",
      "|1979|   983.2565997888067| 35.45406546990496|\n",
      "|1980|    984.497641509434|  36.6627358490566|\n",
      "|1981|   985.9777777777778|  30.7979797979798|\n",
      "|1982|   981.8138195777351|41.602687140115165|\n",
      "|1983|   985.1509433962265|34.782293178519595|\n",
      "|1984|   985.1949339207049|35.401982378854626|\n",
      "|1985|   989.1456815816857| 32.43496357960458|\n",
      "|1986|   985.3704891740176| 34.86367281475541|\n",
      "|1987|   978.7153679653679| 43.52272727272727|\n",
      "|1988|     988.74715261959|  32.2380410022779|\n",
      "|1989|   983.5480116391852| 37.59456838021339|\n",
      "|1990|   981.2694198623402| 40.08849557522124|\n",
      "|1991|   978.0804416403786| 43.71845425867508|\n",
      "|1992|   981.2928679817906| 39.55993930197268|\n",
      "|1993|   987.3385321100917| 33.41284403669725|\n",
      "|1994|   983.7460857726345| 38.35602450646699|\n",
      "|1995|   987.7876712328767| 33.50684931506849|\n",
      "|1996|   983.5871886120997|36.837188612099645|\n",
      "+----+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "df_intensity = df_intensity.withColumn(\"year\", df_intensity[\"year\"].cast(IntegerType()))\n",
    "df_intensity = df_intensity.withColumn(\"avg_central_pressure\", df_intensity[\"avg_central_pressure\"].cast(\"double\"))\n",
    "df_intensity = df_intensity.withColumn(\"avg_wind_speed\", df_intensity[\"avg_wind_speed\"].cast(\"double\"))\n",
    "df_intensity = df_intensity.filter(df_intensity.year >= 1977)\n",
    "df_intensity.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|year|    avg_wind_speed|\n",
      "+----+------------------+\n",
      "|1977|31.975546975546976|\n",
      "|1978| 34.02416356877323|\n",
      "|1979| 35.45406546990496|\n",
      "|1980|  36.6627358490566|\n",
      "|1981|  30.7979797979798|\n",
      "|1982|41.602687140115165|\n",
      "|1983|34.782293178519595|\n",
      "|1984|35.401982378854626|\n",
      "|1985| 32.43496357960458|\n",
      "|1986| 34.86367281475541|\n",
      "+----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_intensiy_wind = df_intensity.filter(df_intensity.avg_wind_speed.isNotNull()).drop('avg_central_pressure')\n",
    "df_intensiy_wind.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|year|avg_central_pressure|\n",
      "+----+--------------------+\n",
      "|1977|   986.1454311454312|\n",
      "|1978|   986.0371747211896|\n",
      "|1979|   983.2565997888067|\n",
      "|1980|    984.497641509434|\n",
      "|1981|   985.9777777777778|\n",
      "|1982|   981.8138195777351|\n",
      "|1983|   985.1509433962265|\n",
      "|1984|   985.1949339207049|\n",
      "|1985|   989.1456815816857|\n",
      "|1986|   985.3704891740176|\n",
      "+----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, avg_central_pressure: double]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_intensity_pressure = df_intensity.drop('avg_wind_speed')\n",
    "df_intensity_pressure.show(10)\n",
    "df_intensity_pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for central pressure prediction: 3.473577220150774\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "#预测强度的回归模型\n",
    "# year作为输入，输出强度\n",
    "# 提取特征向量\n",
    "assembler = VectorAssembler(inputCols=[\"year\"], outputCol=\"features\")\n",
    "df_intensity_features = assembler.transform(df_intensity_pressure)\n",
    "\n",
    "#划分训练集和测试集\n",
    "pressure_train, pressure_test = df_intensity_features.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "#训练\n",
    "pressure_model = LinearRegression(featuresCol=\"features\", labelCol=\"avg_central_pressure\", regParam=0.1)\n",
    "lr_model_central_pressure = pressure_model.fit(pressure_train)\n",
    "\n",
    "#预测\n",
    "pressure_prediction = lr_model_central_pressure.transform(pressure_test)\n",
    "\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"avg_central_pressure\", metricName=\"rmse\")\n",
    "rmse_central_pressure = evaluator.evaluate(pressure_prediction)\n",
    "\n",
    "print(f\"RMSE for central pressure prediction: {rmse_central_pressure}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----------------+\n",
      "|year|features|       prediction|\n",
      "+----+--------+-----------------+\n",
      "|2015|[2015.0]|983.6482701040463|\n",
      "|2016|[2016.0]|983.6388986396908|\n",
      "|2017|[2017.0]|983.6295271753352|\n",
      "|2018|[2018.0]|983.6201557109796|\n",
      "|2019|[2019.0]| 983.610784246624|\n",
      "|2020|[2020.0]|983.6014127822684|\n",
      "|2021|[2021.0]| 983.592041317913|\n",
      "|2022|[2022.0]|983.5826698535574|\n",
      "|2023|[2023.0]|983.5732983892018|\n",
      "|2024|[2024.0]|983.5639269248462|\n",
      "|2025|[2025.0]|983.5545554604906|\n",
      "|2026|[2026.0]| 983.545183996135|\n",
      "|2027|[2027.0]|983.5358125317795|\n",
      "|2028|[2028.0]|983.5264410674239|\n",
      "|2029|[2029.0]|983.5170696030683|\n",
      "|2030|[2030.0]|983.5076981387128|\n",
      "|2031|[2031.0]|983.4983266743573|\n",
      "|2032|[2032.0]|983.4889552100017|\n",
      "+----+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 预测\n",
    "future_years = spark.createDataFrame([(year,) for year in range(2015, 2033)], [\"year\"])\n",
    "future_years_features = assembler.transform(future_years)\n",
    "pressure_predictions = lr_model_central_pressure.transform(future_years_features)\n",
    "pressure_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for wind speed prediction: 3.987021392568346\n"
     ]
    }
   ],
   "source": [
    "# 提取特征向量\n",
    "assembler_wind = VectorAssembler(inputCols=[\"year\"], outputCol=\"features\")\n",
    "df_intensiy_wind_features = assembler_wind.transform(df_intensiy_wind)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "wind_train, wind_test = df_intensiy_wind_features.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# 训练\n",
    "wind_model = LinearRegression(featuresCol=\"features\", labelCol=\"avg_wind_speed\",regParam=0.1)\n",
    "lr_model_wind_speed = wind_model.fit(wind_train)\n",
    "\n",
    "# 预测\n",
    "wind_prediction = lr_model_wind_speed.transform(wind_test)\n",
    "\n",
    "evaluator_wind = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"avg_wind_speed\", metricName=\"rmse\")\n",
    "rmse_wind_speed = evaluator_wind.evaluate(wind_prediction)\n",
    "\n",
    "print(f\"RMSE for wind speed prediction: {rmse_wind_speed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+------------------+\n",
      "|year|features|        prediction|\n",
      "+----+--------+------------------+\n",
      "|2015|[2015.0]|  37.5224634705423|\n",
      "|2016|[2016.0]|  37.5457055616171|\n",
      "|2017|[2017.0]|  37.5689476526919|\n",
      "|2018|[2018.0]| 37.59218974376669|\n",
      "|2019|[2019.0]| 37.61543183484149|\n",
      "|2020|[2020.0]| 37.63867392591629|\n",
      "|2021|[2021.0]| 37.66191601699108|\n",
      "|2022|[2022.0]| 37.68515810806588|\n",
      "|2023|[2023.0]| 37.70840019914068|\n",
      "|2024|[2024.0]| 37.73164229021547|\n",
      "|2025|[2025.0]| 37.75488438129027|\n",
      "|2026|[2026.0]| 37.77812647236507|\n",
      "|2027|[2027.0]|37.801368563439866|\n",
      "|2028|[2028.0]| 37.82461065451466|\n",
      "|2029|[2029.0]|37.847852745589456|\n",
      "|2030|[2030.0]|37.871094836664255|\n",
      "|2031|[2031.0]| 37.89433692773905|\n",
      "|2032|[2032.0]|37.917579018813846|\n",
      "+----+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "future_years = spark.createDataFrame([(year,) for year in range(2015, 2033)], [\"year\"])\n",
    "future_years_features = assembler_wind.transform(future_years)\n",
    "wind_predictions = lr_model_wind_speed.transform(future_years_features)\n",
    "wind_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:===============>(63 + 1) / 64][Stage 20:==>             (8 + 56) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+--------------------+\n",
      "|year|predicted_pressure|predicted_wind_speed|\n",
      "+----+------------------+--------------------+\n",
      "|2015| 983.6482701040463|    37.5224634705423|\n",
      "|2016| 983.6388986396908|    37.5457055616171|\n",
      "|2017| 983.6295271753352|    37.5689476526919|\n",
      "|2018| 983.6201557109796|   37.59218974376669|\n",
      "|2019|  983.610784246624|   37.61543183484149|\n",
      "|2020| 983.6014127822684|   37.63867392591629|\n",
      "|2021|  983.592041317913|   37.66191601699108|\n",
      "|2022| 983.5826698535574|   37.68515810806588|\n",
      "|2023| 983.5732983892018|   37.70840019914068|\n",
      "|2024| 983.5639269248462|   37.73164229021547|\n",
      "|2025| 983.5545554604906|   37.75488438129027|\n",
      "|2026|  983.545183996135|   37.77812647236507|\n",
      "|2027| 983.5358125317795|  37.801368563439866|\n",
      "|2028| 983.5264410674239|   37.82461065451466|\n",
      "|2029| 983.5170696030683|  37.847852745589456|\n",
      "|2030| 983.5076981387128|  37.871094836664255|\n",
      "|2031| 983.4983266743573|   37.89433692773905|\n",
      "|2032| 983.4889552100017|  37.917579018813846|\n",
      "+----+------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 合并两个预测结果\n",
    "combined_predictions = pressure_predictions.select(\"year\", \"prediction\").withColumnRenamed(\"prediction\", \"predicted_pressure\") \\\n",
    "    .join(wind_predictions.select(\"year\", \"prediction\").withColumnRenamed(\"prediction\", \"predicted_wind_speed\"), on=\"year\", how=\"inner\")\n",
    "\n",
    "combined_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "combined_predictions.coalesce(1).write.mode(\"overwrite\").option(\"header\",True).csv(\"result/intensity_prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单台风预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_single_pre=spark.read.option(\"header\", True).csv(r\"../typhoon_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, International number ID: string, year: string, month: string, day: string, hour: string, grade: string, Latitude of the center: string, Longitude of the center: string, Central pressure: string, Maximum sustained wind speed: string, Direction of the longest radius of 50kt winds or greater: string, The longeast radius of 50kt winds or greater: string, The shortest radius of 50kt winds or greater: string, Direction of the longest radius of 30kt winds or greater: string, The longeast radius of 30kt winds or greater: string, The shortest radius of 30kt winds or greater: string, Indicator of landfall or passage: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_single_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, concat_ws\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "# 将数据类型转换为合适的类型\n",
    "df_single_pre = df_single_pre.withColumn(\n",
    "    \"date\",\n",
    "    to_timestamp(concat_ws(\"-\", col(\"year\"), col(\"month\"), col(\"day\"), col(\"hour\")), \"yyyy-MM-dd-HH\")\n",
    ")\n",
    "df_single_pre = df_single_pre.select(\"International number ID\",\"date\", \"Latitude of the center\", \"Longitude of the center\", \"Central pressure\")\n",
    "# 获取时间戳\n",
    "df_single_pre = df_single_pre.withColumn(\"timestamp\", unix_timestamp(\"date\"))\n",
    "\n",
    "df_single_pre = df_single_pre.withColumn(\"Latitude of the center\", df_single_pre[\"Latitude of the center\"].cast(\"double\")/10.)\n",
    "df_single_pre = df_single_pre.withColumn(\"Longitude of the center\", df_single_pre[\"Longitude of the center\"].cast(\"double\")/10.)\n",
    "df_single_pre = df_single_pre.withColumn(\"Central pressure\", df_single_pre[\"Central pressure\"].cast(\"double\"))\n",
    "# 按照台风ID进行分组\n",
    "typhoon_groups = df_single_pre.groupBy(\"International number ID\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+----------------------+-----------------------+----------------+---------+\n",
      "|International number ID|               date|Latitude of the center|Longitude of the center|Central pressure|timestamp|\n",
      "+-----------------------+-------------------+----------------------+-----------------------+----------------+---------+\n",
      "|                   9001|1990-01-12 00:00:00|                   7.0|                  153.0|          1002.0|632073600|\n",
      "|                   9001|1990-01-12 06:00:00|                   7.0|                  152.5|          1000.0|632095200|\n",
      "|                   9001|1990-01-12 12:00:00|                   8.0|                  151.5|          1000.0|632116800|\n",
      "|                   9001|1990-01-12 18:00:00|                   8.5|                  150.5|           998.0|632138400|\n",
      "|                   9001|1990-01-13 00:00:00|                   9.0|                  149.5|           998.0|632160000|\n",
      "|                   9001|1990-01-13 06:00:00|                   9.7|                  148.2|           996.0|632181600|\n",
      "|                   9001|1990-01-13 12:00:00|                  11.1|                  146.7|           996.0|632203200|\n",
      "|                   9001|1990-01-13 18:00:00|                  12.6|                  145.6|           994.0|632224800|\n",
      "|                   9001|1990-01-14 00:00:00|                  13.1|                  145.6|           990.0|632246400|\n",
      "|                   9001|1990-01-14 06:00:00|                  13.8|                  145.5|           985.0|632268000|\n",
      "|                   9001|1990-01-14 12:00:00|                  14.5|                  145.5|           980.0|632289600|\n",
      "|                   9001|1990-01-14 18:00:00|                  15.0|                  145.6|           980.0|632311200|\n",
      "|                   9001|1990-01-15 00:00:00|                  15.5|                  145.8|           980.0|632332800|\n",
      "|                   9001|1990-01-15 06:00:00|                  16.1|                  146.1|           980.0|632354400|\n",
      "|                   9001|1990-01-15 12:00:00|                  16.6|                  146.2|           980.0|632376000|\n",
      "|                   9001|1990-01-15 18:00:00|                  17.1|                  146.2|           980.0|632397600|\n",
      "|                   9001|1990-01-16 00:00:00|                  18.3|                  146.4|           985.0|632419200|\n",
      "|                   9001|1990-01-16 06:00:00|                  19.7|                  147.0|           985.0|632440800|\n",
      "|                   9001|1990-01-16 12:00:00|                  21.1|                  147.9|           990.0|632462400|\n",
      "|                   9001|1990-01-16 18:00:00|                  22.5|                  149.0|           996.0|632484000|\n",
      "|                   9001|1990-01-17 00:00:00|                  24.0|                  151.0|           996.0|632505600|\n",
      "|                   9001|1990-01-17 06:00:00|                  26.0|                  155.0|           992.0|632527200|\n",
      "|                   9001|1990-01-17 12:00:00|                  30.0|                  160.0|           996.0|632548800|\n",
      "|                   9001|1990-01-17 18:00:00|                  31.0|                  166.0|           996.0|632570400|\n",
      "+-----------------------+-------------------+----------------------+-----------------------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "typhoon_id=\"9001\"\n",
    "typhoon_data = df_single_pre.filter(col(\"International number ID\") == typhoon_id)\n",
    "typhoon_data.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lead\n",
    "\n",
    "#最后一个时间点的数据\n",
    "last_recode=typhoon_data.orderBy(\"timestamp\", ascending=False).limit(1)\n",
    "\n",
    "# 获得下一个时间点的位置和强度\n",
    "window_spec = Window.partitionBy(\"International number ID\").orderBy(\"date\")\n",
    "typhoon_data = typhoon_data.withColumn(\"next_latitude\", lead(\"Latitude of the center\").over(window_spec))\n",
    "typhoon_data = typhoon_data.withColumn(\"next_longitude\", lead(\"Longitude of the center\").over(window_spec))\n",
    "typhoon_data = typhoon_data.withColumn(\"next_central_pressure\", lead(\"Central pressure\").over(window_spec))\n",
    "typhoon_data = typhoon_data.dropna(subset=[\"next_latitude\", \"next_longitude\", \"next_central_pressure\"])\n",
    "\n",
    "\n",
    "typhoon_data = typhoon_data.filter(col(\"next_latitude\").isNotNull())\n",
    "#预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+----------------------+-----------------------+----------------+---------+\n",
      "|International number ID|               date|Latitude of the center|Longitude of the center|Central pressure|timestamp|\n",
      "+-----------------------+-------------------+----------------------+-----------------------+----------------+---------+\n",
      "|                   9001|1990-01-17 18:00:00|                  31.0|                  166.0|           996.0|632570400|\n",
      "+-----------------------+-------------------+----------------------+-----------------------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "last_recode.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+----------------------+-----------------------+----------------+---------+-------------+--------------+---------------------+\n",
      "|International number ID|               date|Latitude of the center|Longitude of the center|Central pressure|timestamp|next_latitude|next_longitude|next_central_pressure|\n",
      "+-----------------------+-------------------+----------------------+-----------------------+----------------+---------+-------------+--------------+---------------------+\n",
      "|                   9001|1990-01-12 00:00:00|                   7.0|                  153.0|          1002.0|632073600|          7.0|         152.5|               1000.0|\n",
      "|                   9001|1990-01-12 06:00:00|                   7.0|                  152.5|          1000.0|632095200|          8.0|         151.5|               1000.0|\n",
      "|                   9001|1990-01-12 12:00:00|                   8.0|                  151.5|          1000.0|632116800|          8.5|         150.5|                998.0|\n",
      "|                   9001|1990-01-12 18:00:00|                   8.5|                  150.5|           998.0|632138400|          9.0|         149.5|                998.0|\n",
      "|                   9001|1990-01-13 00:00:00|                   9.0|                  149.5|           998.0|632160000|          9.7|         148.2|                996.0|\n",
      "|                   9001|1990-01-13 06:00:00|                   9.7|                  148.2|           996.0|632181600|         11.1|         146.7|                996.0|\n",
      "|                   9001|1990-01-13 12:00:00|                  11.1|                  146.7|           996.0|632203200|         12.6|         145.6|                994.0|\n",
      "|                   9001|1990-01-13 18:00:00|                  12.6|                  145.6|           994.0|632224800|         13.1|         145.6|                990.0|\n",
      "|                   9001|1990-01-14 00:00:00|                  13.1|                  145.6|           990.0|632246400|         13.8|         145.5|                985.0|\n",
      "|                   9001|1990-01-14 06:00:00|                  13.8|                  145.5|           985.0|632268000|         14.5|         145.5|                980.0|\n",
      "|                   9001|1990-01-14 12:00:00|                  14.5|                  145.5|           980.0|632289600|         15.0|         145.6|                980.0|\n",
      "|                   9001|1990-01-14 18:00:00|                  15.0|                  145.6|           980.0|632311200|         15.5|         145.8|                980.0|\n",
      "|                   9001|1990-01-15 00:00:00|                  15.5|                  145.8|           980.0|632332800|         16.1|         146.1|                980.0|\n",
      "|                   9001|1990-01-15 06:00:00|                  16.1|                  146.1|           980.0|632354400|         16.6|         146.2|                980.0|\n",
      "|                   9001|1990-01-15 12:00:00|                  16.6|                  146.2|           980.0|632376000|         17.1|         146.2|                980.0|\n",
      "|                   9001|1990-01-15 18:00:00|                  17.1|                  146.2|           980.0|632397600|         18.3|         146.4|                985.0|\n",
      "|                   9001|1990-01-16 00:00:00|                  18.3|                  146.4|           985.0|632419200|         19.7|         147.0|                985.0|\n",
      "|                   9001|1990-01-16 06:00:00|                  19.7|                  147.0|           985.0|632440800|         21.1|         147.9|                990.0|\n",
      "|                   9001|1990-01-16 12:00:00|                  21.1|                  147.9|           990.0|632462400|         22.5|         149.0|                996.0|\n",
      "|                   9001|1990-01-16 18:00:00|                  22.5|                  149.0|           996.0|632484000|         24.0|         151.0|                996.0|\n",
      "|                   9001|1990-01-17 00:00:00|                  24.0|                  151.0|           996.0|632505600|         26.0|         155.0|                992.0|\n",
      "|                   9001|1990-01-17 06:00:00|                  26.0|                  155.0|           992.0|632527200|         30.0|         160.0|                996.0|\n",
      "|                   9001|1990-01-17 12:00:00|                  30.0|                  160.0|           996.0|632548800|         31.0|         166.0|                996.0|\n",
      "+-----------------------+-------------------+----------------------+-----------------------+----------------+---------+-------------+--------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "typhoon_data.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "\n",
    "# 提取特征向量\n",
    "assembler_location = VectorAssembler(inputCols=[\"timestamp\", \"Latitude of the center\", \"Longitude of the center\",\n",
    "                                                \"Central pressure\"], outputCol=\"features\")\n",
    "typhoon_data_features = assembler_location.transform(typhoon_data)\n",
    "\n",
    "# 训练模型\n",
    "latitude_model = LinearRegression(featuresCol=\"features\", labelCol=\"next_latitude\", regParam=0.1)\n",
    "lr_single_pre_latitude = latitude_model.fit(typhoon_data_features)\n",
    "\n",
    "# 训练模型\n",
    "longitude_model = LinearRegression(featuresCol=\"features\", labelCol=\"next_longitude\", regParam=0.1)\n",
    "lr_single_pre_longitude = longitude_model.fit(typhoon_data_features)\n",
    "\n",
    "pressure_model = LinearRegression(featuresCol=\"features\", labelCol=\"next_central_pressure\", regParam=0.1)\n",
    "lr_single_pre_pressure = pressure_model.fit(typhoon_data_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_recode = assembler_location.transform(last_recode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+----------------------+-----------------------+----------------+---------+--------------------+------------------+-------------------+--------------------------+\n",
      "|International number ID|               date|Latitude of the center|Longitude of the center|Central pressure|timestamp|            features|predicted_latitude|predicted_longitude|predicted_central_pressure|\n",
      "+-----------------------+-------------------+----------------------+-----------------------+----------------+---------+--------------------+------------------+-------------------+--------------------------+\n",
      "|                   9001|1990-01-17 18:00:00|                  31.0|                  166.0|           996.0|632570400|[6.325704E8,31.0,...|33.231401653063585|  172.9574220958981|         1000.903918070846|\n",
      "+-----------------------+-------------------+----------------------+-----------------------+----------------+---------+--------------------+------------------+-------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 预测未来的位置和强度\n",
    "predictions = lr_single_pre_latitude.transform(last_recode) \\\n",
    "    .withColumnRenamed(\"prediction\", \"predicted_latitude\")\n",
    "\n",
    "predictions = lr_single_pre_longitude.transform(predictions) \\\n",
    "    .withColumnRenamed(\"prediction\", \"predicted_longitude\")\n",
    "\n",
    "predictions = lr_single_pre_pressure.transform(predictions) \\\n",
    "    .withColumnRenamed(\"prediction\", \"predicted_central_pressure\")\n",
    "\n",
    "predictions.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+----------------------+-----------------------+-----------------+---------+\n",
      "|International number ID|               date|Latitude of the center|Longitude of the center| Central pressure|timestamp|\n",
      "+-----------------------+-------------------+----------------------+-----------------------+-----------------+---------+\n",
      "|                   9001|1990-01-18 00:00:00|    33.231401653063585|      172.9574220958981|1000.903918070846|632592000|\n",
      "+-----------------------+-------------------+----------------------+-----------------------+-----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, from_unixtime\n",
    "\n",
    "# 修改\n",
    "predictions = predictions.withColumn(\"date\", col(\"date\") + expr(\"INTERVAL 6 HOURS\"))\n",
    "predictions = predictions.withColumn(\"timestamp\", unix_timestamp(col(\"date\")))\n",
    "\n",
    "predictions = predictions.withColumn(\"Latitude of the center\", col(\"predicted_latitude\")) \\\n",
    "    .withColumn(\"Longitude of the center\", col(\"predicted_longitude\")) \\\n",
    "    .withColumn(\"Central pressure\", col(\"predicted_central_pressure\"))\n",
    "predictions = predictions.select(\"International number ID\",\"date\",\"Latitude of the center\",\"Longitude of the center\",\"Central pressure\",\"timestamp\")\n",
    "predictions.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+----------------------+-----------------------+------------------+---------+\n",
      "|International number ID|               date|Latitude of the center|Longitude of the center|  Central pressure|timestamp|\n",
      "+-----------------------+-------------------+----------------------+-----------------------+------------------+---------+\n",
      "|                   9001|1990-01-18 00:00:00|    33.231401653063585|      172.9574220958981| 1000.903918070846|632592000|\n",
      "|                   9001|1990-01-18 06:00:00|     35.97998259422093|     181.45605981910722|1007.5304003121637|632613600|\n",
      "|                   9001|1990-01-18 12:00:00|     39.28653466335345|     191.86018671195103|1015.9752030709315|632635200|\n",
      "|                   9001|1990-01-18 18:00:00|    43.219083852964104|     204.59116244062574|1026.4695233951097|632656800|\n",
      "|                   9001|1990-01-19 00:00:00|     47.87439519360487|     220.14449125080813|1039.3650741703968|632678400|\n",
      "+-----------------------+-------------------+----------------------+-----------------------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "def predict(df_single_pre,typhoon_id,k=5):\n",
    "    typhoon_data = df_single_pre.filter(col(\"International number ID\") == typhoon_id)\n",
    "    #最后一个时间点的数据\n",
    "    last_recode=typhoon_data.orderBy(\"timestamp\", ascending=False).limit(1)\n",
    "    # last_recode.show()\n",
    "    # 获得下一个时间点的位置和强度\n",
    "    window_spec = Window.partitionBy(\"International number ID\").orderBy(\"date\")\n",
    "    typhoon_data = typhoon_data.withColumn(\"next_latitude\", lead(\"Latitude of the center\").over(window_spec))\n",
    "    typhoon_data = typhoon_data.withColumn(\"next_longitude\", lead(\"Longitude of the center\").over(window_spec))\n",
    "    typhoon_data = typhoon_data.withColumn(\"next_central_pressure\", lead(\"Central pressure\").over(window_spec))\n",
    "    typhoon_data = typhoon_data.dropna(subset=[\"next_latitude\", \"next_longitude\", \"next_central_pressure\"])\n",
    "\n",
    "\n",
    "    typhoon_data = typhoon_data.filter(col(\"next_latitude\").isNotNull())\n",
    "    \n",
    "    # 提取特征向量\n",
    "    assembler_location = VectorAssembler(inputCols=[\"timestamp\", \"Latitude of the center\", \"Longitude of the center\",\"Central pressure\"], outputCol=\"features\")\n",
    "    typhoon_data_features = assembler_location.transform(typhoon_data)\n",
    "\n",
    "    # 训练模型\n",
    "    latitude_model = LinearRegression(featuresCol=\"features\", labelCol=\"next_latitude\", regParam=0.1)\n",
    "    lr_single_pre_latitude = latitude_model.fit(typhoon_data_features)\n",
    "\n",
    "    # 训练模型\n",
    "    longitude_model = LinearRegression(featuresCol=\"features\", labelCol=\"next_longitude\", regParam=0.1)\n",
    "    lr_single_pre_longitude = longitude_model.fit(typhoon_data_features)\n",
    "\n",
    "    pressure_model = LinearRegression(featuresCol=\"features\", labelCol=\"next_central_pressure\", regParam=0.1)\n",
    "    lr_single_pre_pressure = pressure_model.fit(typhoon_data_features)\n",
    "    \n",
    "    \n",
    "    \n",
    "    last_recodes = spark.createDataFrame([], last_recode.schema)\n",
    "    for i in range(k):\n",
    "        last_recode = assembler_location.transform(last_recode)\n",
    "\n",
    "        # 预测未来的位置和强度\n",
    "        predictions = lr_single_pre_latitude.transform(last_recode) \\\n",
    "            .withColumnRenamed(\"prediction\", \"predicted_latitude\")\n",
    "\n",
    "        predictions = lr_single_pre_longitude.transform(predictions) \\\n",
    "            .withColumnRenamed(\"prediction\", \"predicted_longitude\")\n",
    "\n",
    "        predictions = lr_single_pre_pressure.transform(predictions) \\\n",
    "            .withColumnRenamed(\"prediction\", \"predicted_central_pressure\")\n",
    "                \n",
    "        predictions = predictions.withColumn(\"date\", col(\"date\") + expr(\"INTERVAL 6 HOURS\"))\n",
    "        predictions = predictions.withColumn(\"timestamp\", unix_timestamp(col(\"date\")))\n",
    "\n",
    "        predictions = predictions.withColumn(\"Latitude of the center\", col(\"predicted_latitude\")) \\\n",
    "            .withColumn(\"Longitude of the center\", col(\"predicted_longitude\")) \\\n",
    "            .withColumn(\"Central pressure\", col(\"predicted_central_pressure\"))\n",
    "        predictions = predictions.select(\"International number ID\",\"date\",\"Latitude of the center\",\n",
    "                                         \"Longitude of the center\",\"Central pressure\",\"timestamp\")\n",
    "        last_recodes = last_recodes.union(predictions)\n",
    "        last_recode = predictions\n",
    "    return last_recodes\n",
    "\n",
    "ls=predict(df_single_pre,\"9001\",k=5)\n",
    "ls.show()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "#所有台风数\n",
    "print(df_single_pre.filter(col(\"year\") >= 1990).filter(col(\"Indicator of landfall or passage\")==\"#\").select(\"International number ID\").distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handling id: 9407 0.81%\n",
      "handling id: 9314 1.63%\n",
      "handling id: 9119 2.44%\n",
      "handling id: 9708 3.25%\n",
      "handling id: 9609 4.07%\n",
      "handling id: 9114 4.88%\n",
      "handling id: 9305 5.69%\n",
      "handling id: 9311 6.50%\n",
      "handling id: 9211 7.32%\n",
      "handling id: 9606 8.13%\n",
      "handling id: 9313 8.94%\n",
      "handling id: 9209 9.76%\n",
      "handling id: 9304 10.57%\n",
      "handling id: 9808 11.38%\n",
      "handling id: 9918 12.20%\n",
      "handling id: 9306 13.01%\n",
      "handling id: 9719 13.82%\n",
      "handling id: 9411 14.63%\n",
      "handling id: 9426 15.45%\n",
      "handling id: 9916 16.26%\n",
      "handling id: 9621 17.07%\n",
      "handling id: 9807 17.89%\n",
      "handling id: 9612 18.70%\n",
      "handling id: 9709 19.51%\n",
      "handling id: 9805 20.33%\n",
      "handling id: 14 21.14%\n",
      "handling id: 9210 21.95%\n",
      "handling id: 9117 22.76%\n",
      "handling id: 9514 23.58%\n",
      "handling id: 9707 24.39%\n",
      "handling id: 9810 25.20%\n",
      "handling id: 1512 26.02%\n",
      "handling id: 613 26.83%\n",
      "handling id: 205 27.64%\n",
      "handling id: 1418 28.46%\n",
      "handling id: 1518 29.27%\n",
      "handling id: 415 30.08%\n",
      "handling id: 422 30.89%\n",
      "handling id: 1112 31.71%\n",
      "handling id: 1318 32.52%\n",
      "handling id: 1910 33.33%\n",
      "handling id: 2108 34.15%\n",
      "handling id: 1915 34.96%\n",
      "handling id: 406 35.77%\n",
      "handling id: 1215 36.59%\n",
      "handling id: 310 37.40%\n",
      "handling id: 1109 38.21%\n",
      "handling id: 1821 39.02%\n",
      "handling id: 423 39.84%\n",
      "handling id: 1609 40.65%\n",
      "handling id: 712 41.46%\n",
      "handling id: 1722 42.28%\n",
      "handling id: 1919 43.09%\n",
      "handling id: 410 43.90%\n",
      "handling id: 1004 44.72%\n",
      "handling id: 1906 45.53%\n",
      "handling id: 1721 46.34%\n",
      "handling id: 221 47.15%\n",
      "handling id: 610 47.97%\n",
      "handling id: 1610 48.78%\n",
      "handling id: 1607 49.59%\n",
      "handling id: 421 50.41%\n",
      "handling id: 1106 51.22%\n",
      "handling id: 1616 52.03%\n",
      "handling id: 206 52.85%\n",
      "handling id: 1812 53.66%\n",
      "handling id: 1007 54.47%\n",
      "handling id: 1419 55.28%\n",
      "handling id: 1908 56.10%\n",
      "handling id: 207 56.91%\n",
      "handling id: 1515 57.72%\n",
      "handling id: 1611 58.54%\n",
      "handling id: 1216 59.35%\n",
      "handling id: 1820 60.16%\n",
      "handling id: 1408 60.98%\n",
      "handling id: 1705 61.79%\n",
      "handling id: 418 62.60%\n",
      "handling id: 404 63.41%\n",
      "handling id: 1210 64.23%\n",
      "handling id: 2204 65.04%\n",
      "handling id: 111 65.85%\n",
      "handling id: 709 66.67%\n",
      "handling id: 507 67.48%\n",
      "handling id: 1606 68.29%\n",
      "handling id: 115 69.11%\n",
      "handling id: 1815 69.92%\n",
      "handling id: 2109 70.73%\n",
      "handling id: 416 71.54%\n",
      "handling id: 511 72.36%\n",
      "handling id: 204 73.17%\n",
      "handling id: 2212 73.98%\n",
      "handling id: 1317 74.80%\n",
      "handling id: 213 75.61%\n",
      "handling id: 1612 76.42%\n",
      "handling id: 1217 77.24%\n",
      "handling id: 209 78.05%\n",
      "handling id: 1204 78.86%\n",
      "handling id: 304 79.67%\n",
      "handling id: 1411 80.49%\n",
      "handling id: 1718 81.30%\n",
      "handling id: 116 82.11%\n",
      "handling id: 509 82.93%\n",
      "handling id: 314 83.74%\n",
      "handling id: 1824 84.55%\n",
      "handling id: 514 85.37%\n",
      "handling id: 1009 86.18%\n",
      "handling id: 705 86.99%\n",
      "handling id: 306 87.80%\n",
      "handling id: 607 88.62%\n",
      "handling id: 2214 89.43%\n",
      "handling id: 1115 90.24%\n",
      "handling id: 2208 91.06%\n",
      "handling id: 315 91.87%\n",
      "handling id: 411 92.68%\n",
      "handling id: 704 93.50%\n",
      "handling id: 1810 94.31%\n",
      "handling id: 1511 95.12%\n",
      "handling id: 215 95.93%\n",
      "handling id: 2114 96.75%\n",
      "handling id: 1703 97.56%\n",
      "handling id: 1324 98.37%\n",
      "handling id: 216 99.19%\n",
      "handling id: 918 100.00%\n"
     ]
    }
   ],
   "source": [
    "# 选取有登陆的台风\n",
    "typhoon_ids = df_single_pre.filter(col(\"year\") >= 1990).filter(col(\"Indicator of landfall or passage\")==\"#\").select(\"International number ID\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "\n",
    "counter=0\n",
    "for typhoon_id in typhoon_ids:\n",
    "    counter=counter+1\n",
    "    prog=counter/123*100\n",
    "    print(f\"handling id: {typhoon_id} {prog:.2f}%\")\n",
    "    predictions = predict(df_single_pre, typhoon_id, k=5)\n",
    "    all_predictions = all_predictions.union(predictions)\n",
    "    predictions=predictions.toPandas()\n",
    "    if counter == 1:\n",
    "        predictions.to_csv(\"result/position_predict.csv\", index=False)\n",
    "    else:\n",
    "        with open(\"result/position_predict.csv\", 'a') as f:\n",
    "            predictions.to_csv(f, header=False, index=False)\n",
    "    # 释放之前使用的变量空间\n",
    "    del predictions\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
